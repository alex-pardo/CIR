\section{Implementation}
\label{sec:implementation}

\subsection{Collision Avoidance}
We use the depth map given by the RGB-D camera from the robot to analyse whether there is collision risk. We use a sliding window to compute the minimum average distance among the patches (Eq. \ref{eq:closest}) and get the approximate distance to the closest visible object to the robot. 
\begin{equation}\label{eq:closest}
\min\frac{\sum_{w,h}^{i,j}depth(p_{i,j})}{w*h} < threshold\ \ \forall p \in Img
\end{equation}
By setting two thresholds the robot sends a warning to the teleoperator, when there is an object close enough, or stops, when there is an object too close. Once the robot is freeze because of an obstacle the teleoperator can still move backwards (at its own risk) or turn around. \\

In order to tackle the \textbf{Freezing Robot Problem} we use a time window to store the closest distance and check if it disappears in time, i.e. if the sum of the distances in that window frame is larger that certain threshold then we consider that the object is a danger, otherwise the obstacle is just passing by and so not a real danger.\\

Since stop the robot abruptly might scare or disturb the people around we decided to \textbf{reduce the speed} of the robot once it enters to a warning zone inverse proportionally to the closest distance until it reaches the stop zone where it stops. We also setted the backwards speed to half the forward speed since there is no camera in the back and the teleoperator will not be aware of any obstacle. 

\subsection{Gesture Robot Control}
For the \emph{Gesture Robot Control} we use a Kinect camera setted in the teleoperator side. It has two main phases, \emph{depth image processing} and \emph{gesture recognition}.\\

\begin{itemize}
	\item \textbf{Depth Image Processing}: We use OpenNI sdk to extract the skeleton of the teleoperator. OpenNI skeleton tracking algorithm is not easy to find, however the Microsoft approach is to use a Random Forest classifier trained using a labelled dataset of depth images. \\
	
	The result is an efficient algorithm that works in real time.
	
	\item \textbf{Gesture Recognition}: We use a gesture recognition system based on right hand positions in the space (3D) and its variation over time, i.e. three axis and its two possible directions, which do 6 different gestures. The movement is controlled by an empirically tested threshold.\\
		
	Moving the hand in the depth axis will command to the robot to move forward or backward depending on the direction of the hand's movement, moving the hand in the horizontal axis will send to the robot the order of turning right or left and finally the vertical axis will send an stop signal to the robot.
	
\end{itemize}

\subsection{Communication}

The \textbf{Anti-collision system} code runs on the server side and is able to stop the MASHI when an obstacle is close to the robot autonomously. Also blocking the user to move the MASHI forward if the obstacle still in the path. It also controls the speed of the robot as described before.\\

The \textbf{Gestural Control} code runs in the user's machine, able to track the skeleton and to send the commands to the robot server via a websocket connection.

