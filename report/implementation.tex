\section{Implementation}
\label{sec:implementation}

\subsection{Collision Avoidance}
We use the depth map given by the RGB-D camera from the robot to analyse whether there is collision risk. We use a sliding window to compute the minimum average distance among the patches (Eq. \ref{eq:closest}) and get the approximate distance to the closest visible object to the robot. 
\begin{equation}\label{eq:closest}
\min\frac{\sum_{w,h}^{i,j}depth(p_{i,j})}{w*h} < threshold\ \ \forall p \in Img
\end{equation}
By setting two thresholds the robot sends a warning to the teleoperator, when there is an object close enough, or stops, when there is an object too close. Once the robot is freeze because of an obstacle the teleoperator can still move backwards (at its own risk) or turn around. \\

In order to tackle the \textbf{Freezing Robot Problem} we use a time window to store the closest distance and check if it disappears in time, i.e. if the sum of the distances in that window frame is larger that certain threshold, then we consider that the object is a danger, otherwise the obstacle is just passing by and so not a real danger.\\

Since stop the robot abruptly might scare or disturb the people around we decided to \textbf{reduce the speed} of the robot once it enters to a warning zone inverse proportionally to the closest distance until it reaches the stop zone where it stops. We also setted the backwards speed to half the forward speed since there is no camera in the back and the teleoperator will not be aware of any obstacle. \\

\subsection{Gesture Robot Control}
For the \emph{Gesture Robot Control} we use a Kinect camera setted in the teleoperator side. It has two main phases, \emph{depth image processing} and \emph{gesture recognition}.\\

\begin{itemize}
	\item \textbf{Depth Image Processing}: We use OpenNI sdk to extract the skeleton of the teleoperator. OpenNI skeleton tracking algorithm is not easy to find, however the Microsoft approach is to use a Random Forest classifier trained using a labelled dataset of depth images. \\
	
	The result is an efficient algorithm that works in real time.
	
	\item \textbf{Gesture Recognition}: We use a gesture recognition system based on right hand positions in the space (3D) and its variation over time, i.e. three axis and its two possible directions, which do 6 different gestures. The movement is controlled by an empirically tested threshold.\\
		
	Moving the hand in the depth axis will command to the robot to move forward or backward depending on the direction of the hand's movement, moving the hand in the horizontal axis will send to the robot the order of turning right or left and finally the vertical axis will send an stop signal to the robot.
	
\end{itemize}

\subsection{Communication}


All the connections on the system are shown in Figure \ref{fig:com}, boxes in white represent those parts that are new. Next we detail how we added the two new modules:
\begin{itemize}
\item The \textbf{Anti-collision system} code runs on the MASHI side since it should be able to stop it even if the communication channel is broken. This part of the system runs on the Robot controller (which is a Server). The distance obtained is broadcasted over the network in order to allow everyone to know it. This part of the system also controls the speed reduction when approaching an obstacle as described before.

\item On the other side, we have the \textbf{Gestural Control} which runs on the teleoperator machine. The application uses Openni and Nite libraries so it must be written in C++ while all the system is implemented in node-js. In order to enable the communication between languages, we defined a Client-Server protocol inside Kinect Server, which is getting data from the gesture recognition system (Gestures Camera in the figure) and sending it to the teleoperator application. Then, this will send the proper commands to the Server. The C++ - node js transition is made inside Kinect Server which is implemented using C++ but binded on node js using node-gyp (i.e. C++ code is executed from node js)

\end{itemize}

This complex structure allows us to maintain as much as possible the original communication since we are adding the newest modules (Gesture recognition) at the client side, making it transparent for the server placed on the robot. The collision avoidance system is set on the robot server because it does not require any communication with the clients.

\begin{figure}[htb!]
\centering
\includegraphics[width=0.8 \textwidth]{images/communication.png}
\caption{System communication diagram}
\label{fig:com}
\end{figure}
